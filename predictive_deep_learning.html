<!DOCTYPE html>
<HTML>
<HEAD> <TITLE>PREDICTIONS</TITLE> </HEAD>

<h1>What does the future look like?</h1>
<p>Predicting semantic segmentation frames of traffic environments.</br>
  The data is obtained using the <a href="http://carla.org">CARLA</a> simulator at 5 fps.<p>
<IMG SRC="img/indicator.png">

<h2>Samples</h2>
<p>Conditioned on 10 initial frames. Image dimensions are 3x64x64<p>

<BODY>
  <IMG SRC="gifs/model_illustration/model_cropped.gif"></br>
  <IMG SRC="gifs/model_illustration/pred.gif"></br>
  </br></br>
  <IMG SRC="gifs/seg/sequence_0.gif">
  <IMG SRC="gifs/seg/sequence_1.gif">
  <IMG SRC="gifs/seg/sequence_2.gif">
  </br>
  <IMG SRC="gifs/seg/sequence_3.gif">
  <IMG SRC="gifs/seg/sequence_4.gif">
  <IMG SRC="gifs/seg/sequence_5.gif">
  </br>
  <IMG SRC="gifs/seg/sequence_6.gif">
  <IMG SRC="gifs/seg/sequence_7.gif">
  <IMG SRC="gifs/seg/sequence_8.gif">
  </br>

  <h2>11.11.2019 Evaluation</h2>
  <p>
    Mean IoU from a selection of classes. The measurements are from 192 sequences with a confidence</br>
    interval of 95%. Some critical classes, like vehicles and pedestrians, are barely represented. This</br>
    might be due to the fact that they seldom occur in the training set, and when they do they usually</br>
    contain only a few pixels. The mean IoU over all classes is still strong since there are a few dominating</br>
    classes which are predicted very well.
  </p>
  <IMG SRC="img/iou_data.png">
    <p>
      The thought behind this plot/metric is to investigate how far into the future it is convenient</br>
      (useful) to predict what will happen. As expected, the mean IoU decreases over time, meaning early</br>
      predictions are more accurate.
    </p>

  <h2>13.11.2019 What next?</h2>
  <p>
    -Should I investigate to what extent the number of conditional frames aids the predictions?</br></br>
    -The model completely ignores pedestrians. What might help?</br>
    &emsp;&emsp;&emsp;increase image size | train end-to-end | add more pedestrians in the training data</br></br>
    -Could it be an idea to evaluate main events of the predictions, rather than comparing individual frames?</br>
    If so, how? Ideas -> Classify events:</br>
    &emsp;&emsp;&emsp;right turn | left turn | intersection | pedestrian crossing</br></br>
  </p>


  <h2>15.11.2019 Update</h2>
  <p>
    Currently working on getting the same to work on larger images (128x128). VAE reconstructions seem to better</br>
    model pedestrians and other small objects. I also tried swapping LSTM cells with GRUs, but this worsened the</br>
    results. Maybe due to the long sequence lengths, and small gradients?</br></br>

    Prediction evaluation of main events based on crowdsourcing seems like a suitable option. Though, wait with</br>
    planning this until the RGB model is implemented and verified.</br></br>

    I should find a way to compare my results to that of <a href="https://arxiv.org/abs/1703.07684">Luc et. al (2017)</a>. My predictions both short term and</br>
    long term seem to be more accurate. Long term predictions in the paper actually appear like messy blends of blobs,</br>
    rather than actual predictions. My predictions, however, make sure that structural integrity of objects are retained.</br>
    Comparing with the results of the paper would either mean to somehow get hold of the annotated <a href="https://www.cityscapes-dataset.com/">Cityscapes</a> dataset</br>
    and reproduce their results, or training their model on my data from CARLA.</br></br>

    <i>Predicting Deeper into the Future of Semantic Segmentation</i> <a href="https://github.com/facebookresearch/SegmPred">Implementation on Github</a>.
  </p>




  <h2>19.11.2019 Update (scratching this for now)</h2>
  <p>
    I tried generating some predictions with image size 128x128, but they were very inaccurate, nearly all frames</br>
    were similar (little movement among objects). However, the average IoU was still high, which basically just confirms</br>
    that IoU is not an appropriate measure for evaluation of predictions. Reducing batch size (from 64 to 8) made no</br>
    difference, so I don't think it happens because the model averages many predictions into one. After some more investigation,</br>
    I have (almost) concluded that since my VAE has a powerful decoder, it suffered from <i>posterior collapse</i>. The latent</br>
    space is of size 4x4, and the decoded image is 128x128 (1024x compression), and it seems the decoder ignored the latent</br>
    codes altogether. I propose a new VAE setup that utilizes the latent codes through three steps of decoding, and early</br>
    experiments with this setup yields promising results. It makes sense that this can work, since the decoder is not allowed</br>
    to forget about the latent codes. The encoding can be seen as a single step, but I leave it as is for now.
  </p>
  <IMG SRC="img/3_step_vqvae.png" height="300" width="225">

  <h2>26.11.2019 Update</h2>
  <p>
    Did a quick traning and sampling of RGB sequences. The image reconstruction is not exactly great, but the training set </br>
    (which corresponds to the one used for segmentation) is quite small. I believe the RGB model variant might need more training</br>
    examples in order to better reconstruct the details.</br></br>
    These samples are also conditioned on 10 initial frames. Image dimensions are 3x64x64
    <p>

    <BODY>
      <IMG SRC="gifs/rgb/sequence_0.gif">
      <IMG SRC="gifs/rgb/sequence_1.gif">
      <IMG SRC="gifs/rgb/sequence_2.gif">
      </br>
      <IMG SRC="gifs/rgb/sequence_3.gif">
      <IMG SRC="gifs/rgb/sequence_4.gif">
      <IMG SRC="gifs/rgb/sequence_5.gif">
      </br>
      <IMG SRC="gifs/rgb/sequence_6.gif">
      <IMG SRC="gifs/rgb/sequence_7.gif">
      <IMG SRC="gifs/rgb/sequence_8.gif">
      </br>
    </BODY>

  <h2>More about the process</h2>
  <p>
    <a href="https://docs.google.com/document/d/19_Y2DlCs2vWuI22xg-mo3rGZnUE2KNIqzgJBAneiWR0/edit?usp=sharing">Veiledningsreferater</a>
  </p>

</BODY>


</HTML>
